\chapter{Introduction}
\section{Project Motivation}
A decade ago, systems with multiple processors were expensive and relatively rare; only developers with highly specialized skills could successfully parallelize server and scientific applications to exploit the power of multiprocessor systems. In the past few years, multicore systems have become pervasive, and more programmers want to employ parallelism to wring the most performance out of their applications.

Exploiting multiple cores introduces new correctness and performance problems such as data races, deadlocks, load balancing, and false sharing. Old problems such as memory corruption become more difficult because parallel programs can be nondeterministic. Programmers need a deeper understanding of their software's dynamic behavior to successfully make the transition from single to multiple threads and processes.

Instrumentation is one tool for collecting the information needed to understand programs. Instrumentation-based tools typically insert extra code into a program to record events during execution.[1]–[2][3][4] The cost of executing the extra code can be as low as a few cycles, enabling fine-grained observation down to the instruction level.

Pin[2] (www.pintool.org) is a software system that performs runtime binary instrumentation of Linux and Microsoft Windows applications. Pin's aim is to provide an instrumentation platform for building a wide variety of program analysis tools, called pintools. By performing the instrumentation on the binary at runtime, Pin eliminates the need to modify or recompile the application's source and supports the instrumentation of programs that dynamically generate code. \\
\section{Aims and Objectives}
In this report, we have identified a number of design goals for such an advisor service. First, given an incoming query workload and a performance goal, the service should provide solutions for executing a given workload on a cloud database. Each solution should indicate: (a) the cloud resources to be provisioned (e.g., number/type of VMs), (b) the distribution of resources among the workload queries (e.g., which VM will execute a given query), and (c) the execution order of these queries. We refer to these solutions collectively as workload schedules.

We also need a customizable service that supports equally diverse application-defined performance criteria. We envision a customizable service that generates workload schedules tuned to performance goals and workload characteristics specified by the application. Supported metrics should capture the performance of individual queries (e.g., query latency) as well as the performance of batch workloads (e.g., max query latency of an incoming query workload).

Finally, since cloud providers offer resources for some cost (i.e., price/hour for renting a VM), optimizing schedules for this cost is vital for cloud-based applications. Hence, any workload management advisor should be cost-aware. Cost functions are available through contracts between the service providers and their customers in the form of service level agreements (SLAs). These cost functions define the price for renting cloud resources, the performance goals, and the penalty to be paid if the agreed-upon performance is not met. A workload management service should consider all these cost factors while assisting applications in exploring performance/cost trade-offs.

In this project we try to build a system for cloud databases designed to satisfy the above requirements. Our system offers customized solutions to the workload management problem by recommending cost-effective strategies for executing incoming workloads for a given application. These strategies are expressed as decision-tree models/neural network models  or any machine learning model as proposed and  utilizes a supervised learning framework to “learn” models customized to the application’s performance goals and workload characteristics. For an incoming workload, our system can parse the model to identify the number/type of VMs to provision, the assignment of queries to VMs, and the execution order within each VM, in order to execute the workload and meet the performance objective with low-cost. Each model is cost-aware: it is trained on a set of performance and cost-related features collected from minimum cost schedules of sample workloads. This cost accounts for resource provisioning as well as any penalties paid due to failure to meet the performance goals. Furthermore, our proposed features are independent from the application’s performance goal and workload specification, which allows our system to learn effective models for a range of metrics (e.g., average/max latency, percentile-based metrics). Given an incoming batch workload and a trained model, our system parses the model and returns a low cost schedule for executing the workload on cloud resources. 
